{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Computational Methods in Economics\n",
    "\n",
    "## Lecture 4 - Root Finding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last update: 2018-11-13 19:57:41.592749\n"
     ]
    }
   ],
   "source": [
    "# Author: Alex Schmitt (schmitt@ifo.de)\n",
    "\n",
    "import datetime\n",
    "print('Last update: ' + str(datetime.datetime.today()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## Preliminaries\n",
    "\n",
    "#### Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn\n",
    "\n",
    "import scipy.optimize\n",
    "\n",
    "# import sys\n",
    "from importlib import reload"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## This Lecture\n",
    "\n",
    "- [Introduction](#introduction)\n",
    "- [Bisection](#bisection)\n",
    "- [Function Iteration](#funiter)\n",
    "- [Newton's Method](#newton)\n",
    "- [Numerical Differentiation](#numdiff)\n",
    "- [Quasi-Newton Methods](#quasi)\n",
    "- [Convergence](#convergence)\n",
    "- [The Scipy Package](#scipy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "--------------------------------------------------------------------------------------------------------------------------------\n",
    "<a id = \"introduction\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Introduction\n",
    "\n",
    "A function $f(x)$ has a *root* (also called a *zero*) at $x^*$ if $f(x^*) = 0$. Two cases are relevant:\n",
    "\n",
    "- $f$ can be a univariate scalar/real-valued function $f: \\mathbb{R}\\ \\rightarrow \\mathbb{R}$, i.e. both input and output are scalars, or both its range and its domain have a dimension of 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- $f$ can be a vector-valued function $\\mathbf{f}: \\mathbb{R}^n\\ \\rightarrow \\mathbb{R}^n$, i.e. both its range and its domain have a dimension greater than 1. In this case, finding the roots of a vector-valued function is equivalent to *solving a system of nonlinear equations*. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The intermediate case of a multivariate scalar function $f: \\mathbb{R}^n\\ \\rightarrow \\mathbb{R}$ - its inputs are vectors, hence its domain has a dimension greater than 1 - is going to be important in the next lecture. \n",
    "\n",
    "Finding the root(s) of a function is one of the most common computational problems in economics, often applied when looking for an equilibrium. In other words, an equilibrium is usually defined by a set of equations, as illustrated by the following example.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Example: Neoclassical Growth Model\n",
    "\n",
    "Recall the standard NGM that you have seen in Macro:\n",
    "\n",
    "- Utility function:\n",
    "\n",
    "\\begin{equation}\n",
    "    u(c, h) = \\frac{c^{1-\\nu}}{1-\\nu} - B \\frac{h^{1+\\eta}}{1+\\eta}\n",
    "\\end{equation}\n",
    "\n",
    "with $c$ denoting consumption and $h$ labor supply."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Production function:\n",
    "\n",
    "\\begin{equation}\n",
    "    f(k, h) = A k^\\alpha h^{1-\\alpha}\n",
    "\\end{equation}\n",
    "\n",
    "with $k$ denoting the capital stock, and $A$ the productivity level."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Resource Constraint:\n",
    "\n",
    "\\begin{equation}\n",
    "    k_{t+1} + c_t = f(k_t, h_t) + (1 - \\delta) k_t = A k_t^\\alpha h_t^{1-\\alpha} + (1 - \\delta) k_t\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Planner's Problem:\n",
    "\n",
    "\\begin{equation}\n",
    "    \\max_{\\left\\{c_t, k_{t+1}, h_t\\right\\}} \\sum^\\infty_{t = 0} \\beta^t u(c_t, h_t) \n",
    "\\end{equation}\n",
    "\n",
    "s.t. the resource constraint."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "##### First-order conditions\n",
    "\n",
    "(1) Euler equation\n",
    "\n",
    "\\begin{equation}\n",
    "    c^{-\\nu} = \\beta \\left[ (c')^{-\\nu} (f_k(k', h') + 1 - \\delta) \\right]    \n",
    "\\end{equation}\n",
    "\n",
    "(2) intratemporal optimality condition\n",
    "\n",
    "\\begin{equation}\n",
    "    B h^{\\eta} = c^{-\\nu} f_h(k, h)  \n",
    "\\end{equation}\n",
    "\n",
    "where I have used the notation $c = c_t$ and $c' = c_{t+1}$ (and analogous for $k$ and $h$) for brevity. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "##### Steady State\n",
    "\n",
    "In an equilibrium, the two first-order conditions, combined with the resource constraint, must hold in every period. We will get to how to solve for the full dynamic allocation later in this course. \n",
    "\n",
    "For now, let's consider the *steady state*, where all variables are constant over time, i.e. $c_t = c_{t+1} = c_s$ and so on. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The Euler equation then can be simplied to:\n",
    "\n",
    "\\begin{equation}\n",
    "    1 = \\beta \\left[f_k(k_s, h_s) + 1 - \\delta \\right]    \n",
    "\\end{equation}\n",
    "\n",
    "For the intratemporal optimality condition, use the resource constraint to substitute for consumption:\n",
    "\n",
    "\\begin{equation}\n",
    "    B h_s^{\\eta} = \\left[ f(k_s, h_s) - \\delta k_s \\right]^{-\\nu} f_h(k_s, h_s)  \n",
    "\\end{equation}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "This is a nonlinear system of two equations, with two unknown variables, $k_s$ and $h_s$, which can be solved using the methods introduced below. We can define a vector-valued function $\\mathbf{S}$ with\n",
    "\n",
    "\\begin{equation}\n",
    "   \\mathbf{S}(k, h) = \n",
    "    \\left[\n",
    "    \\begin{array}{c}\n",
    "        \\beta \\left[f_k(k, h) + 1 - \\delta \\right]  - 1 \\\\\n",
    "        \\left[ f(k, h) - \\delta k \\right]^{-\\nu} f_h(k, h) - B h^{\\eta}\n",
    "    \\end{array}\n",
    "    \\right]\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Finding the steady state of the model then requires finding a root of function $\\mathbf{S}$, i.e. a vector $(k_s, h_s)$ such that \n",
    "\n",
    "\\begin{equation}\n",
    "   \\mathbf{S}(k_s, h_s) = \n",
    "    \\left[\n",
    "    \\begin{array}{c}\n",
    "        0 \\\\\n",
    "        0\n",
    "    \\end{array}\n",
    "    \\right]\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Some Preliminaries\n",
    "\n",
    "- For a multivariate real-valued function $f: \\mathbb{R}^n\\ \\rightarrow \\mathbb{R}$, the vector consisting of the first derivatives is called the *gradient* (vector):\n",
    "\n",
    "\\begin{equation}\n",
    " \\nabla f(\\mathbf{x}) = \\left[\n",
    "\\begin{matrix}\n",
    " \\partial f/ \\partial x_1 \\\\\n",
    " \\vdots \\\\\n",
    "  \\partial f/ \\partial x_n \n",
    "\\end{matrix}  \\right]\n",
    "\\end{equation}\n",
    "\n",
    "where $\\mathbf{x}$ is an n-by-1 vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- For a vector-valued function $\\mathbf{f}: \\mathbb{R}^n\\ \\rightarrow \\mathbb{R}^n$, the *Jacobian* (i.e., the matrix of the first derivatives), is defined as \n",
    "\n",
    "\\begin{equation}\n",
    " J(\\mathbf{x}) = \\left[\n",
    "\\begin{matrix}\n",
    " \\partial f_1/ \\partial x_1 & ... & \\partial f_1/ \\partial x_n \\\\\n",
    " \\vdots & \\ddots & \\vdots \\\\\n",
    "  \\partial f_n/ \\partial x_1 & ... & \\partial f_n/ \\partial x_n \n",
    "\\end{matrix}  \\right]\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Using the gradient notation, we can also write this as\n",
    "\n",
    "\\begin{equation}\n",
    " J(\\mathbf{x}) = \\left[\n",
    "\\begin{matrix}\n",
    "  \\nabla f_1(\\mathbf{x})^T \\\\\n",
    " \\vdots  \\\\\n",
    "  \\nabla f_n(\\mathbf{x})^T\n",
    "\\end{matrix}  \\right]\n",
    "\\end{equation}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Taylor Series and Taylor's Formula\n",
    "\n",
    "For a univariate function $f$ that is $n$ times continuously differentiable, a *Taylor series* or *Taylor approximation* around $x_0$ is given by:\n",
    "\n",
    "\\begin{equation}\n",
    "    f(x) \\approx f(x_0) + f'(x_0) (x - x_0) + \\frac{1}{2} f''(x_0) (x - x_0)^2 + ... + \\frac{1}{n!} f^{(n)}(x_0) (x - x_0)^n \n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Closely related to this is *Taylor's Theorem*: if $f$ is, for example, twice continuously differentiable in an interval that contains $x$ and $x_0$, then \n",
    "\n",
    "\\begin{equation}\n",
    "    f(x) = f(x_0) + f'(x_0) (x - x_0) + \\frac{1}{2} f''(c) (x - x_0)^2 \n",
    "\\end{equation}\n",
    "\n",
    "for some number $c$ between $x$ and $x_0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "For a multivariate function $f: \\mathbb{R}^n\\ \\rightarrow \\mathbb{R}$, we can state Taylor's Theorem in the following way: if $f$ is twice continuously differentiable and $p \\in \\mathbb{R}^n$, we have that\n",
    "\n",
    "\\begin{equation}\n",
    "    f(\\mathbf{x}_0 + \\mathbf{p}) = f(\\mathbf{x}_0) + \\nabla f(\\mathbf{x}_0)^{T} \\mathbf{p} + \\frac{1}{2} \\mathbf{p}^T \\nabla^2 f(\\mathbf{x}_0 + t\\mathbf{p}) \\mathbf{p} \n",
    "\\end{equation}\n",
    "\n",
    "for some $t \\in (0,1)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "--------------------------------------------------------------------------------------------------------------------------------\n",
    "<a id = 'bisection'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Bisection\n",
    "\n",
    "The simplest way to compute the root of a continuous univariate real-valued function is the *bisection method*. While simple, bisection captures two important features of most root-finding and optimization methods: it is a *local* method and it is based on an *iterative procedure*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The key idea behind the bisection method is based on the *Intermediate Value Theorem*: if $f$ is continuous and defined on the interval $[a,b]$, and if $f(a)$ and $f(b)$ are distinct values, then $f$ must assume all values in between. \n",
    "\n",
    "Since we are interested in where $f$ assumes the value 0, we need $f(a)$ and $f(b)$ to have *different signs*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "## cp. figure\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The bisection method implements the following \"pseudo-code\":\n",
    "\n",
    "(i) Start with two distinct values $a$ and $b$, $a < b$, such that $f(a)$ and $f(b)$ are defined and have different signs, i.e. $f(a) \\cdot f(b) < 0$. Moreover, specify a \"tolerance level\" $tol$ which should be a very small number, e.g. 1e-8.\n",
    "\n",
    "(ii) Compute the midpoint between $a$ and $b$, $x = \\frac{a + b}{2}$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "(iii) If $f(x)$ has the same sign as $f(a)$, replace the left endpoint of the interval with $x$, i.e. $a = x$.\n",
    "\n",
    "(iv) If $f(x)$ has the same sign as $f(b)$, replace the right endpoint of the interval with $x$, i.e. $b = x$.\n",
    "\n",
    "(v) Check the *stopping rule*: if the absolute value of $f(x)$ is less than a *tolerance level*, $|f(x)| < tol$, stop and report the solution at $x$. If not, go back to (ii) and repeat."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Bisection is an *iterative procedure*: \n",
    "   - At the beginning of each iteration step, the interval $[a,b]$ contains a root of $f$. The interval is then divided (\"bisected\") into two subintervals of equal length. One of the two subintervals must contain the root, and hence have endpoints of different signs. \n",
    "\n",
    "   - This subinterval is taken as the interval $[a,b]$ used for the next iteration. This process continues until the function value of the midpoint $x$ of the current interval is sufficiently close to 0.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Moreover, bisection is a *local* method: it will not give you all the roots of a function, but only one of the roots (in case there are multiple roots) between $a$ and $b$. \n",
    "\n",
    "A corollary of this is that the outcome of bisection (and of local methods in general) is sensitive to the starting point chosen by the user, here the values for $a$ and $b$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The bisection method is robust in the sense that it will find a root in a known number of iterations, assuming the initial choices for $a$ and $b$ lead to different signs for $f(a)$ and $f(b)$. \n",
    "\n",
    "The obvious downside of bisection is that it only works for univariate functions. Moreover, it is usually slower than the other methods discussed below. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In this week's problem set, you will be asked to code up the bisection method. \n",
    "\n",
    "Of course, most programming languages already have in-built implementations (e.g. in SciPy: **scipy.optimize.bisect**, as discussed below), so writing your own function may seem a bit redundant, but will help you to get used to the inner workings of many of the algorithms used in scientific computing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "--------------------------------------------------------------------------------------------------------------------------------\n",
    "<a id = 'funiter'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Function Iteration\n",
    "\n",
    "We have started to talk about iterative methods at the end of last lecture. To recap, the basic idea of iterative methods is to generate a sequence of approximations to the object of interest, e.g. the solution to linear or nonlinear system of equations, following an iteration rule: \n",
    "\n",
    "\\begin{equation}\n",
    "    x^{(k+1)} = g( x^{(k)} ),\n",
    "\\end{equation}\n",
    "\n",
    "where $k$ is an indicator counting the number of iterations. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Hence, the value for $x$ in the $k+1$-iteration is obtained by applying function $g$ on the value for $x$ in the $k$-iteration. Ideally, these approximations become more and more \"precise\" with an increasing number of iterations. \n",
    "\n",
    "Put differently, we aim for *convergence*: formally, a sequence $\\left\\{ x^{(k)} \\right\\}_{k = 0}^\\infty$ converges if\n",
    "\n",
    "\\begin{equation}\n",
    "    x^{(k)} \\ \\rightarrow\\ x^* \\quad \\text{as} \\quad k\\ \\rightarrow\\ \\infty \n",
    "\\end{equation}\n",
    "\n",
    "Recall that iterative methods, in contrast to direct methods, do not yield an exact solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "When finding the root of a function $f$ or solving for a system of nonlinear equations with function iteration, the functional form of $g$ is simply\n",
    "\n",
    "\\begin{equation}\n",
    "    g( x ) = x - f(x).\n",
    "\\end{equation}\n",
    "\n",
    "This is intuitive: at the root $ x = x^* $, we have $f(x^*) = 0$ and hence $g (x^*) = x^*$. In other words, $x^*$ is a *fixed point*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The following piece of code implements function iteration. As a simple workhorse example, consider the function\n",
    "\n",
    "\\begin{equation}\n",
    "    f(x) = 4 \\ln(x) - 4,\n",
    "\\end{equation}\n",
    "\n",
    "which has a root at $x = e^1 = 2.718282$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "## define functions\n",
    "def fun(x):\n",
    "    return 4*np.log(x) - 4\n",
    "\n",
    "def g(x):\n",
    "    return x - fun(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Before running the algorithm, we need to define a few parameters. As in the case of iterative methods in the previous lecture, we have to provide a starting value (\"initial guess\") for $x$.\n",
    "\n",
    "Moreover, we also have to choose a tolerance level **tol** for the stopping rule. As for the stopping rule itself, we terminate the algorithm if $f(x^{(k)})$ is close to 0, and and hence if $x^{(k+1)} = g(x^{(k)})$ is close to $x^{(k)}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "## initial guess for x\n",
    "x = 4\n",
    "## tolerance level for stopping rule\n",
    "tol = 1e-8\n",
    "## for illustration: iteration counter and list to store results\n",
    "it = 0\n",
    "lst = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "For illustration, we print the current guess for $x^{(k)}$ for each iteration. As we can see,  $x^{(k)}$ converges to $x^*$ as the number of iterations increases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4548225555204377\n",
      "2.862604636225108\n",
      "2.6556769468169183\n",
      "2.748878575829132\n",
      "2.70410642422237\n",
      "2.725020362315921\n",
      "2.7151167603287156\n",
      "2.7197769279002424\n",
      "2.717577467326751\n",
      "2.718614081555206\n",
      "2.7181251950992964\n",
      "2.7183556905115984\n",
      "2.7182470026657746\n",
      "2.7182982497675234\n",
      "2.7182740855933667\n",
      "2.7182854793739897\n",
      "2.7182801069913993\n",
      "2.718282640162424\n",
      "2.718281445726661\n",
      "2.718282008924203\n",
      "2.718281743366526\n",
      "2.718281868581682\n",
      "2.71828180954051\n",
      "2.718281837379471\n",
      "2.718281824252906\n",
      "Number of iterations = 25\n"
     ]
    }
   ],
   "source": [
    "while abs((x - g(x))) > tol: \n",
    "    it += 1\n",
    "    ## apply g function and store and print results\n",
    "    x = g(x)  \n",
    "    lst.append(x)\n",
    "    print(x)\n",
    "    \n",
    "\n",
    "print(\"Number of iterations = {}\".format(it) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We can Python's matplotlib package to illustrate how function iteration works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0,0.5,'$g(x)$')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = np.linspace(0.9, 3, 100)\n",
    "fig, ax = plt.subplots(2,1, sharex = True)\n",
    "# ax[0].ylabel($$f(x)$$)\n",
    "ax[0].plot(x, fun(x))\n",
    "ax[0].hlines(0, 0.9, 3)\n",
    "ax[1].plot(x, g(x))\n",
    "ax[1].plot(x, x, '--')\n",
    "ax[1].set_xlabel('$x$')\n",
    "ax[0].set_ylabel('$f(x)$')\n",
    "ax[1].set_ylabel('$g(x)$')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "--------------------------------------------------------------------------------------------------------------------------------\n",
    "<a id = 'newton'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Newton's Method\n",
    "\n",
    "Most algorithms used in practice to find the roots of a nonlinear system of equations are based on Newton's method. \n",
    "\n",
    "As function iteration, it is an iterative method. However, it uses additional information, namely about the derivative(s) of $f$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Univariate Function\n",
    "\n",
    "Let us start with the case of a univariate function $f$. Recall that we want to find a root $x^*$ of the function $f$, i.e. where $f(x^*) = 0$. Start with an initial guess for $x^*$, denoted by $x_0$. We can approximate $f$ with a first-order Taylor approximation around $x_0$:\n",
    "\n",
    "\\begin{equation}\n",
    "    f(x) \\approx f(x_0) + (x - x_0) f'(x_0)\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Setting $f(x) = 0$ - our target value - and solving this expression for $x$ gives us the \"best guess\" for $x^*$ given the initial guess and the properties of the function (i.e. its value and derivative) at $x_0$:\n",
    "\n",
    "\\begin{equation}\n",
    "x^* \\approx x_0 - \\frac{f(x_0)}{f'(x_0)} \\rightarrow  x^{(1)}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "Iterating on this step, we can again generate a sequence $x^{(1)}, x^{(2)}, ..., x^{(n)}$; hence the iteration rule is given by:\n",
    "\n",
    "\\begin{equation}\n",
    " x^{(k+1)} = x^{(k)} - \\frac{f(x^{(k)})}{f'(x^{(k)})}\n",
    "\\end{equation}\n",
    "\n",
    "In other words, the functional form of $g$ is now\n",
    "\n",
    "\\begin{equation}\n",
    "    g( x ) = x - \\frac{f(x)}{f'(x)}.\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The key idea of Newton's method is *successive linerarization*: a nonlinear problem is replaced with a sequence of linear problems whose solutions converge to the solution of a nonlinear problem. This is illustrated by the following figure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![graph_newton.png](attachment:graph_newton.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Comparing to simple function iteration above, we have one additional term, the derivative of $f$ at $x^{(k)}$. Hence, we use *more information on the properties of the function* than above. \n",
    "\n",
    "More precisely, we put a weight on the distance between the old guess $x^{(k)}$ and the new guess $x^{(k+1)}$. With function iteration, this weight was given by $f(x^{(k)})$, while in Newton's method, it is $f(x^{(k)})/f'(x^{(k)})$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "It is intuitive why this is an improvement:\n",
    "- if the absolute value of $f'(x^{(k)})$ is small, this means the function is relatively flat at $x^{(k)}$; in this case, it is likely that the current guess $x^{(k)}$ is still far from the root, and hence the jump to the next guess should be large\n",
    "- if the absolute value of $f'(x^{(k)})$ is large, the function is relatively steep, making it more likely that we are close to  the root; hence the jump to the next guess should be small"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Newton's method implements the following pseudo-code:\n",
    "\n",
    "(i) Specify tolerance levels $tol1$ and $tol2$ and choose a starting guess $x^{(0)}$.\n",
    "\n",
    "(ii) Compute the next iterate as \n",
    "\n",
    "\\begin{equation}\n",
    " x^{(k+1)} = x^{(k)} - \\frac{f(x^{(k)})}{f'(x^{(k)})}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "(iii) Check the stopping rule: if $|x^{(k+1)}- x^{(k)}| < tol1$, stop. If not, go back to (ii).\n",
    "\n",
    "(iv) If $|f(x^{(k+1)})| < tol2$, report $x^{(k+1)}$ as the solution. Otherwise, report failure. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The following code implements Newton's method. As before, the current guess for $x$ is printed in every iteration. \n",
    "\n",
    "Unsurprisingly given the intuition above, Newton's method needs considerably fewer iterations than simple function iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def fd(x):\n",
    "    \"\"\"\n",
    "    Computes the derivative for the example function 4*np.log(x) - 4\n",
    "    \"\"\"\n",
    "    return 4/x\n",
    "\n",
    "def g_newton(fun, fun_d, x):\n",
    "    \"\"\"\n",
    "    Implements the iteration rule for Newton's method. \n",
    "    \"\"\"\n",
    "    f, fd = fun(x), fun_d(x)\n",
    "    return x - f * fd**(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def my_newton(fun, fun_d, x, tol1 = 1e-8, tol2 = 1e-8):\n",
    "    \"\"\"\n",
    "    Implements Newton's method. \n",
    "    \"\"\"\n",
    "    eps = 1\n",
    "    it = 0\n",
    "    \n",
    "    while eps > tol1:\n",
    "        it += 1\n",
    "        x_new = g_newton(fun, fun_d, x)\n",
    "        eps = abs(x - x_new)\n",
    "        x = x_new\n",
    "        print(x_new)\n",
    "    \n",
    "    print(\"Number of iterations = {}\".format(it) )\n",
    "    \n",
    "    if abs(fun(x)) < tol2: \n",
    "        return x\n",
    "    else:\n",
    "        print(\"No solution found!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0\n",
      "2.613705638880109\n",
      "2.7162439263557903\n",
      "2.718281064358138\n",
      "2.7182818284589376\n",
      "2.7182818284590455\n",
      "Number of iterations = 6\n"
     ]
    }
   ],
   "source": [
    "x_root = my_newton(fun, fd, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The stopping criteria should not be set too loosely. For some functions, a higher $tol1$ does not impact the solution from Newton's method: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0\n",
      "2.613705638880109\n",
      "2.7162439263557903\n",
      "2.718281064358138\n",
      "2.7182818284589376\n",
      "Number of iterations = 5\n"
     ]
    }
   ],
   "source": [
    "x_root = my_newton(fun, fd, 1, tol1 = 1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "For other functions, however, setting $tol1$ too high can result in finding a \"root\" quite far away from zero. This is true for functions that are quite flat around its root. For these functions, we also see very slow convergence. Note that in the example below, $f(x) = x^6$, a small $tol2$ does not compensate for a rather high $tol1$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def fun2(x):\n",
    "    return x**6\n",
    "def fd2(x):\n",
    "    return 6 * x**(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1a1aca7550>]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = np.linspace(-1, 1, 100)\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(x, fun2(x))\n",
    "# ax.plot(x, fd2(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8333333333333334\n",
      "0.6944444444444444\n",
      "0.5787037037037037\n",
      "0.48225308641975306\n",
      "0.4018775720164609\n",
      "0.3348979766803841\n",
      "0.2790816472336535\n",
      "0.2325680393613779\n",
      "0.19380669946781492\n",
      "0.16150558288984576\n",
      "0.13458798574153813\n",
      "0.11215665478461512\n",
      "0.09346387898717926\n",
      "0.07788656582264938\n",
      "0.06490547151887449\n",
      "0.05408789293239541\n",
      "0.04507324411032951\n",
      "0.03756103675860792\n",
      "0.031300863965506596\n",
      "0.02608405330458883\n",
      "0.021736711087157357\n",
      "0.018113925905964463\n",
      "0.015094938254970386\n",
      "0.01257911521247532\n",
      "0.010482596010396101\n",
      "0.008735496675330084\n",
      "0.00727958056277507\n",
      "0.006066317135645892\n",
      "0.00505526427970491\n",
      "0.004212720233087425\n",
      "0.0035106001942395207\n",
      "0.002925500161866267\n",
      "0.0024379168015552224\n",
      "0.002031597334629352\n",
      "0.0016929977788577933\n",
      "0.0014108314823814943\n",
      "0.0011756929019845785\n",
      "0.0009797440849871487\n",
      "0.0008164534041559572\n",
      "0.0006803778367966309\n",
      "0.0005669815306638591\n",
      "0.00047248460888654926\n",
      "Number of iterations = 42\n",
      "1.1125665436700811e-20\n"
     ]
    }
   ],
   "source": [
    "x_root = my_newton(fun2, fd2, 1, tol1 = 1e-4, tol2 = 1e-10)\n",
    "print(fun2(x_root))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Multivariate Case\n",
    "\n",
    "The logic from the univariate case translates to a vector-valued function $\\mathbf{f}: \\mathbb{R}^n\\ \\rightarrow \\mathbb{R}^n$.  Recall that its Jacobian is defined as:\n",
    "\n",
    "\\begin{equation}\n",
    " J(\\mathbf{x}) = \\left[\n",
    "\\begin{matrix}\n",
    " \\partial f_1/ \\partial x_1 & ... & \\partial f_1/ \\partial x_n \\\\\n",
    " \\vdots & \\ddots & \\vdots \\\\\n",
    "  \\partial f_n/ \\partial x_1 & ... & \\partial f_n/ \\partial x_n \n",
    "\\end{matrix}  \\right]\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Start with a first-order Taylor approximation around $\\mathbf{x}_0$:\n",
    "\n",
    "\\begin{equation}\n",
    " 0 = \\mathbf{f}(\\mathbf{x}) \\approx \\mathbf{f}(\\mathbf{x}_0) + J(\\mathbf{x}) (\\mathbf{x} - \\mathbf{x}_0)\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "As a side note, to see that a Taylor approximation works also in the case of vector-valued functions, note that we can apply it element-wise, i.e. \n",
    "\n",
    "\\begin{equation}\n",
    " f_j(\\mathbf{x}) \\approx f_j(\\mathbf{x}_0) + f_j(\\mathbf{x})^T (\\mathbf{x} - \\mathbf{x}_0)\n",
    "\\end{equation}\n",
    "\n",
    "and recall from above that\n",
    "\n",
    "\\begin{equation}\n",
    " J(\\mathbf{x}) = \\left[\n",
    "\\begin{matrix}\n",
    "  \\nabla f_1(\\mathbf{x})^T \\\\\n",
    " \\vdots  \\\\\n",
    "  \\nabla f_n(\\mathbf{x})^T\n",
    "\\end{matrix}  \\right]\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Hence,\n",
    "\n",
    "\\begin{equation}\n",
    " \\mathbf{x} \\approx \\mathbf{x}_0 - J^{-1}(\\mathbf{x}_0) \\mathbf{f}(\\mathbf{x}_0)\n",
    "\\end{equation}\n",
    "\n",
    "The key idea is to use this relation iteratively, i.e. generate a sequence $\\mathbf{x}^{(1)}, \\mathbf{x}^{(2)}, ..., \\mathbf{x}^{(m)}$  where\n",
    "\n",
    "\\begin{equation}\n",
    " \\mathbf{x}_{k+1} = \\mathbf{x}_{k} - J^{-1}(\\mathbf{x}_{k}) \\mathbf{f}(\\mathbf{x}_{k})\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "As an example, consider the function\n",
    "\n",
    "\\begin{equation}\n",
    " \\mathbf{f}(\\mathbf{x}) = \\left[\n",
    "\\begin{matrix}\n",
    " x_2^2 - 1 \\\\\n",
    "  \\sin{x_1} - x_2\n",
    "\\end{matrix}  \\right]\n",
    "\\end{equation}\n",
    "\n",
    "To apply Newton's method, start by coding up the function and its Jacobian:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def fun_vv(x):\n",
    "    \"\"\"\n",
    "    Implements a system of equation in two unknowns, here f(x) = [x2**2 - 1; sin(x1) - x2]\n",
    "    \"\"\"\n",
    "    return np.array( (x[1]**2 - 1 , np.sin(x[0]) - x[1] ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def fun_J(x):\n",
    "    \"\"\"\n",
    "    Implements the Jacobian system of equation in two unknowns above\n",
    "    \"\"\"\n",
    "    f_00 = 0\n",
    "    f_01 = 2 * x[1]\n",
    "    f_10 = np.cos(x[0])\n",
    "    f_11 = -1\n",
    "    \n",
    "    return np.array([[f_00, f_01], [f_10, f_11]])   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def my_newton_mult(fun, fun_d, x,  tol = 1e-8):\n",
    "    \"\"\"\n",
    "    Implements Newton's method for a vector-valued function\n",
    "    \"\"\"    \n",
    "    eps = 1\n",
    "    it = 0\n",
    "    while eps > tol:\n",
    "        it += 1\n",
    "        f, J = fun(x), fun_d(x)\n",
    "        x_new = x - np.linalg.inv(J) @ f\n",
    "        eps = np.linalg.norm(x - x_new)\n",
    "        x = x_new\n",
    "    \n",
    "    print(\"Number of iterations = {}\".format(it) )\n",
    "    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of iterations = 24\n",
      "[1.57079633 1.        ]\n",
      "[0. 0.]\n"
     ]
    }
   ],
   "source": [
    "x_init = [1.5,0.9]\n",
    "x = my_newton_mult(fun_vv, fun_J, x_init)\n",
    "print(x)\n",
    "print(fun_vv(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of iterations = 27\n",
      "[ 4.71238897 -1.        ]\n",
      "[0.00000000e+00 1.11022302e-16]\n"
     ]
    }
   ],
   "source": [
    "x_init = [3,-0.8]\n",
    "x = my_newton_mult(fun_vv, fun_J, x_init)\n",
    "print(x)\n",
    "print(fun_vv(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "As bisection, Newton's method is a local method, i.e. does not necessarily find the global optimum. In the case of multiple solutions, like in the example above, it converges to one solution - which one depends on the starting point."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Moreover, it is not guaranteed that the iterates in Newton's method converge, in particular for \"erratic\" functions with high derivatives that change sign frequently. \n",
    "\n",
    "In such cases, Newton's method converges only for initial starting values that are sufficiently close to a root $\\mathbf{x}^*$ (always assuming that the Jacobian $J$ is invertible and well-conditioned at  $\\mathbf{x}^*$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We can modify Newton's algorithm as outlined above to increase the likelihood of convergence, by incorporating *backstepping*. \n",
    "\n",
    "Intuitively, if the sequence of iterates $\\mathbf{x}^{(k)}$ \"goes in the wrong direction\" - that is, if the distance to the root gets larger instead of smaller as $k$ increases - we decrease the size of the \"step\" to the next iterate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "## cp. classroom notes\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "If $J$ is ill-conditioned at  $\\mathbf{x}^*$, it can lead to inaccurately computed updates $\\mathbf{x}^{(k)}$, which may prevent Newton's method from converging. \n",
    "\n",
    "As outlined in the last lecture, ill-conditioning can stem from units of measurement that vary vastly in their order of magnitude. Rescaling variables so that their values have comparable orders of magnitudes may be a remedy here (and is a good idea in general)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "--------------------------------------------------------------------------------------------------------------------------------\n",
    "<a id = \"numdiff\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Numerical Differentiation\n",
    "\n",
    "Before moving on, it is useful to look at *numerical differentiation*: instead of working with precise derivatives of a function, we can use numerical approximations for these derivatives. \n",
    "\n",
    "This is very useful in particular when the function is complicated, and hence its precise derivatives are hard to obtain.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Numerical derivatives are based on *finite differences*. In the one-dimensional case, we have \n",
    "\n",
    "\\begin{equation}\n",
    "    \\frac{\\partial f}{\\partial x} \\approx \\frac{f(x + \\epsilon) - f(x)}{\\epsilon}\n",
    "\\end{equation}\n",
    "\n",
    "where $\\epsilon$ is small. In other words, the first derivative at $x$ is approximated by the response to a small perturbation of $x$. Note that the approximation above is called the *forward-difference* or *one-sided-difference*. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Similarly, the *central-difference* is given by:\n",
    "\n",
    "\\begin{equation}\n",
    "    \\frac{\\partial f}{\\partial x} \\approx \\frac{f(x + \\epsilon)- f(x - \\epsilon)}{2 \\epsilon}\n",
    "\\end{equation}\n",
    "\n",
    "What value should be chosen for $\\epsilon$? A good rule of thumb is the square root of machine epsilon:\n",
    "\n",
    "\\begin{equation}\n",
    "    \\epsilon = \\sqrt{\\epsilon_{DP}} \\approx 10^{-8}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Formally, finite differencing is based on Taylor's Theorem, as shown below for a multivariate function $f: \\mathbb{R}^n\\ \\rightarrow \\mathbb{R}$. In this case, we can write the forward-difference formula as:\n",
    "\n",
    "\\begin{equation}\n",
    "    \\frac{\\partial f}{\\partial x_i} \\approx \\frac{f(\\mathbf{x} + \\epsilon e_i) - f(\\mathbf{x})}{\\epsilon},\n",
    "\\end{equation}\n",
    "\n",
    "where $e_i$ is the corresponding canonical vector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "For a vector-valued function, a similar derivation gives:\n",
    "\n",
    "\n",
    "\\begin{equation}\n",
    "     J(\\mathbf{x}) \\mathbf{p} \\approx \\frac{\\mathbf{f}(\\mathbf{x} + \\epsilon \\mathbf{p}) - \\mathbf{f}(\\mathbf{x})}{\\epsilon}\n",
    "\\end{equation}\n",
    "\n",
    "for a given vector $\\mathbf{p}$. \n",
    "\n",
    "Note that this expression does not give us an approximation for the full Jacobian, but rather for the product $J(\\mathbf{x}) \\mathbf{p}$. It requires two function evaluations; it can be shown that a complete approximation of the Jacobian would require $n + 1$ function evaluations for a function of dimension $n$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### In Python\n",
    "\n",
    "Numerical differentation is helpful when using Newton's method, as outlined above. One common problem with Newton's method are programming errors when coding the Jacobian, in particular for complicated functions. An easy check is to compare the analytic Jacobian with its finite-difference counterpart. \n",
    "\n",
    "Python has a package **statsmodels** that includes a routine for numerical differentation of a vector-valued function, as illustrated by the following example: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.00000000e+00, -2.00000000e+00],\n",
       "       [-1.08160913e-08, -1.00000000e+00]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fun_J(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.00000000e+00, -1.99999999e+00],\n",
       "       [ 2.37159347e-08, -1.00000000e+00]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import statsmodels.api as sm\n",
    "\n",
    "sm.tools.numdiff.approx_fprime(x, fun_vv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "As a side note, the SciPy package has a function of the same name, **scipy.optimize.approx_fprime**. In contrast to the **statsmodels** version, this works only for univariate and multivariate scalar functions; in the multivariate case, this computes the gradient. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.1428571428571428"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## precise derivative of the example function below\n",
    "fd(3.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.1428571])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## numerical derivative\n",
    "scipy.optimize.approx_fprime([3.5], fun, [1e-8])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "--------------------------------------------------------------------------------------------------------------------------------\n",
    "<a id = 'quasi'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Quasi-Newton Methods\n",
    "\n",
    "There is an obvious cost of using Newton's method as outlined above: we need to provide the analytical derivative of a univariate scalar function or the Jacobian of a vector-valued function, respectively. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "While this may be not a big deal for simple functions as in the examples above, for more complicated problems, this step may involve a large cost in terms of time for computing the derivatives and for coding them up. \n",
    "\n",
    "Moreover, as mentioned above, coding up complicated derivatives increases the risk of programming errors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Hence, in practice we often rely on \"derivative-free\" or *Quasi-Newton* methods. In a nutshell, their basic idea is the same as in Newton method's - successive linearization - but instead of using the precise derivatives of a function, we approximate them numerically. \n",
    "\n",
    "A drawback is that we have to provide an initial guess for the function's derivative or Jacobian."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Univariate Functions: Secant Method\n",
    "\n",
    "In the one-dimensional case, an obvious idea would be to obtain the derivative in Newton's method by numerical differentation:\n",
    "\n",
    "\\begin{equation}\n",
    "    f'(x^{(k)}) \\approx \\frac{f(x^{(k)} + \\epsilon) - f(x^{(k)})}{\\epsilon}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "While this would work, in practice we use the *secant method*, where the finite-difference approximation of $f'(x)$ is constructed from the function values at the current and the previous iterate:\n",
    "\n",
    "\\begin{equation}\n",
    "    f'(x^{(k)}) \\approx \\frac{f(x^{(k)}) - f(x^{(k - 1)})}{x^{(k)} - x^{(k-1)}} \\equiv (f')^{(k)}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The main advantage is that this reduces the number of function evaluations: since $f(x^{(k - 1)})$ was evaluated anyway when updating the guess for $x$, it is easy to store and use for the update of $f'(x^{(k)})$. \n",
    "\n",
    "In the first expression, we would have to *additionally* evaluate $f(x^{(k)} + \\epsilon)$ in every iteration. Note that the computational cost may not be large for simple function, but can be substantial for more complicated functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![graph_secant.png](attachment:graph_secant.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Note that the secant method generates not only a sequence $\\left\\{ x^{(k)} \\right\\}$, but also a sequence $\\left\\{(f')^{(k)} \\right\\}$ of approxiations of the derivative. \n",
    "\n",
    "In general, while $\\left\\{ x^{(k)} \\right\\}$ converge to the root of the function, $\\left\\{(f')^{(k)} \\right\\}$ does *not* converge to the derivative of $f$ at the root. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In this week's problem set, you are asked to to implement this function numerically. Intuitively, the secant method requires more iterations and hence more function evaluations than Newton's method due to the approximation of the derivative. \n",
    "\n",
    "For more complex functions, however, this doesn't necessarily translate into a longer running time, as the secant method does not require to evaluate the function's derivative."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Multivariate Functions: Broyden's Method\n",
    "\n",
    "The cost of computing exact analytical derivatives increases quadratically in the number of dimensions. Hence, in practice, we usually rely on the multidimensional equivalent to the secant method, *Broyden's Method*.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Recall that the iteration rule for Newton's method is given by \n",
    "\n",
    "\\begin{equation}\n",
    " \\mathbf{x}^{(k+1)} = \\mathbf{x}^{(k)} - J^{-1}(\\mathbf{x}^{(k)}) \\mathbf{f}(\\mathbf{x}^{(k)})\n",
    "\\end{equation}\n",
    "\n",
    "The idea of Broyden's method is to approximate $J(\\mathbf{x}^{(k)})$ by a matrix $A^{(k)}$. Hence, we use the following iteration rule:\n",
    "\n",
    "\\begin{equation}\n",
    " \\mathbf{x}^{(k+1)} = \\mathbf{x}^{(k)} - \\left(A^{(k)}\\right)^{-1} \\mathbf{f}(\\mathbf{x}^{(k)})\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "As the superscript indicates, we will have to update $A^{(k)}$ in every iteration; in other words, we will get a sequence $\\left\\{A^{(k)}\\right\\}$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Assume we have determined $\\mathbf{x}^{(k+1)}$ from $\\mathbf{x}^{(k)}$ and $A^{(k)}$. How do we get $A^{(k+1)}$? One idea would be to use numerical differentiation for $A^{(k+1)}$. One way to write this would be that the $i$th *column* of the Jacobian can be approximated by\n",
    "\n",
    "\\begin{equation}\n",
    "  J_i(\\mathbf{x}^{(k+1)}) \\approx \\frac{\\mathbf{f}(\\mathbf{x}^{(k+1)} + \\epsilon e_i) - \\mathbf{f}(\\mathbf{x}^{(k+1)})}{\\epsilon} = A_i^{(k + 1)}\n",
    "\\end{equation}\n",
    "\n",
    "where $e_i$ is again the corresponding canonical vector. This would require $n + 1$ evaluations of the function $\\mathbf{f}$, one for each column, plus $\\mathbf{f}(\\mathbf{x}^{(k+1)})$, in every iteration, which can amount to a considerable computational cost. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Instead, recall from above that for a given vector $\\mathbf{p}$, we can approximate the product of the Jacobian of a vector-valued function and $\\mathbf{p}$ by\n",
    "\n",
    "\\begin{equation}\n",
    "     J(\\mathbf{x}) \\mathbf{p} \\approx \\frac{\\mathbf{f}(\\mathbf{x} + \\epsilon \\mathbf{p}) - \\mathbf{f}(\\mathbf{x})}{\\epsilon}\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "Analogous to the one-dimensional case, we will work with an approximation $A$ where $\\epsilon = 1$. Setting $\\mathbf{p}\n",
    "^{(k)} = \\mathbf{x}^{(k+1)} - \\mathbf{x}^{(k)}$, define $A$ such that\n",
    "\n",
    "\\begin{equation}\n",
    " A \\mathbf{p}^{(k)} = \\mathbf{f}(\\mathbf{x}^{(k+1)}) - \\mathbf{f}(\\mathbf{x}^{(k)}).\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "That is, we require a numerical approximation $A$ of the Jacobian to satisfy the *secant condition*. In other words, $J(\\mathbf{x}^{(k+1)})$ and its approximation $A$ have a similar behavior along the direction $\\mathbf{p}^{(k)}$. \n",
    "\n",
    "However, note that there is no unique $A$ that satisfies the secant condition. In fact, the secant condition is a system of $n$ equations in $n^2$ unknowns, i.e. for $n^2$ elements of $A$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "To solve this issue, Broyden's method gets $A^{(k+1)}$ by making the \"smallest possible change\" to $A^{(k)}$, while requiring $A^{(k+1)}$ to satisfy the secant condition. \n",
    "\n",
    "This is achieved by assuming that both $A^{(k+1)}$ and $A^{(k)}$ behave in the same way along directions *orthogonal to* $\\mathbf{p}^{(k})$, i.e. for any vector $\\mathbf{q}$ where $\\mathbf{q}' \\mathbf{p}^{(k)} = 0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In other words, for given $\\mathbf{x}^{(k+1)}$, $\\mathbf{x}^{(k)}$ and $A^{(k)}$, Broyden's method looks for an $A^{(k+1)}$ such that\n",
    "\n",
    "\\begin{equation}\n",
    " A^{(k+1)} \\mathbf{p}^{(k)} = \\mathbf{f}(\\mathbf{x}^{(k+1)}) - \\mathbf{f}(\\mathbf{x}^{(k)}).\n",
    "\\end{equation}\n",
    "\n",
    "and\n",
    "\n",
    "\\begin{equation}\n",
    " A^{(k+1)} \\mathbf{q} = A^{(k)} \\mathbf{q}\\ \\ \\text{for}\\ \\ \\mathbf{q}^{T} \\mathbf{p}^{(k)} = 0\n",
    "\\end{equation}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "You will show in the problem set that these requirements are satisfied by the following update rule for $A^{(k+1)}$:\n",
    "\n",
    "\\begin{equation}\n",
    " A^{(k+1)} = A^{(k)} + \\frac{ \\left( \\mathbf{f}(\\mathbf{x}^{(k+1)}) - \\mathbf{f}(\\mathbf{x}^{(k)}) - A^{(k)} \\mathbf{p}^{(k)} \\right) (\\mathbf{p}^{(k)})^T}{(\\mathbf{p}^{(k)})^T \\mathbf{p}^{(k)}}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Moreover, it can be shown that $A^{(k+1)}$ found in this way achieves the smallest possible change to $A^{(k)}$ in the Euclidean/Frobenius norm among all matrices $A$ that satisfies the secant condition, i.e. \n",
    "\n",
    "\\begin{equation}\n",
    " A^{(k+1)} \\in \\arg \\min_{A :\\ A \\mathbf{p}^{(k)} = \\mathbf{f}(\\mathbf{x}^{(k+1)}) - \\mathbf{f}(\\mathbf{x}^{(k)})} || A - A^{(k)} ||\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "As a final remark, while we could write our implementation of Broyden's method, we will skip this and instead rely on Scipy's implementation, as outlined below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "--------------------------------------------------------------------------------------------------------------------------------\n",
    "<a id = 'convergence'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Convergence\n",
    "\n",
    "All of the methods above were iterative, i.e. generating sequences $x^{(k)}$ (hopefully) converging to the root of the function. Importanly, a sequence of iterates $x^{(k)}$ converges to $x^*$ at a rate of order $p$ if there is a constant $C$ such that\n",
    "\n",
    "\\begin{equation}\n",
    "    || x^{(k+1)} - x^* || \\le C || x^{(k)} - x^* ||^p\n",
    "\\end{equation}\n",
    "\n",
    "for sufficiently large $k$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We can determine $C$ and $p$ for the different methods above:\n",
    "\n",
    "- Bisection converges with $C = 0.5$ and $p = 1$, and hence at a *linear rate*\n",
    "\n",
    "- Function iteration converges also at a linear rate, with $C$ equal to $f'(x^*)$\n",
    "\n",
    "- The secant/Broyden's method converges at a *superlinear rate*, with $1 < p \\approx 1.62 < 2$\n",
    "\n",
    "- Newton's method converges at a *quadratic rate* of $p = 2$\n",
    "\n",
    "Compare the example in M&F, section 3.6."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Summary: What method to pick?\n",
    "\n",
    "- Newton's method has the fastest rate of convergence, but can require a lot of \"developmental\" effort; it should be used for nonlinear equations of small dimensions, when the derivatives are not too hard to find and to code, and when an equation is to be solved many times"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- The secant/Broyden's method has a smaller rate of convergence (but still more than linear) and requires less programming time; it should be used when the derivatives are expensive to compute and code, and the equation is not solved too often\n",
    "\n",
    "- In the univariate case, the bisection method is the most robust; for highly irregular functions, it can be used in combination with Newton's method, to find a close initial guess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "--------------------------------------------------------------------------------------------------------------------------------\n",
    "<a id = 'scipy'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Root Finding in SciPy\n",
    "\n",
    "Modern programming languages have built-in implementations of the algorithms outlined above. In Python, we mainly rely on the SciPy package, which comes with the Anaconda distribution. \n",
    "\n",
    "For rootfinding and numerical optimization (in the next lecture), we use Scipy's subpackage **scipy.optimize**, which we first need to import: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import scipy.optimize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### One dimension\n",
    "\n",
    "For the univariate case, consider again the function \n",
    "\n",
    "\\begin{equation}\n",
    "    f(x) = 4 \\ln(x) - 4\n",
    "\\end{equation}\n",
    "\n",
    "We define the function and use the **bisect()** function, an implementation of the bisection method outlined above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.718281828459567\n"
     ]
    }
   ],
   "source": [
    "def fun(x):\n",
    "    return 4*np.log(x) - 4\n",
    "\n",
    "print(scipy.optimize.bisect(fun,1,4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**bisect(fun,a,b)** takes three arguments: the function **fun** (which can be built-in or user-written), and an upper and lower initial guess for the root. In other words, you tell the algorithm to look for a root in the interval $[a,b]$. \n",
    "\n",
    "The important thing to remember here is that $f(a)$ and $f(b)$ must have different signs - if they do not, you will get an error message (in this case, change $a$ or $b$ and try again).  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In the example above, solving for the root using Python is not really necessary. The real advantage of numerical root finding is in situations where finding a solution to $f(x) = 0$ analytically is not feasible. Consider, for example,\n",
    "\n",
    "\\begin{equation}\n",
    "    f(x) = \\sin(4 (x - 1/4)) + x + x^{20} - 1\n",
    "\\end{equation}\n",
    "\n",
    "We can Matplotlib to plot the function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1c1bae6748>]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def fun(x):\n",
    "    return np.sin(4 * (x - 0.25)) + x + x**20 - 1\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "x = np.linspace(0.3, 1, 50)\n",
    "ax.plot(x, fun(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Finding a root via the bisection method is straightforward:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4082935042806639\n"
     ]
    }
   ],
   "source": [
    "print(scipy.optimize.bisect(fun,0,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The Scipy function **newton(fun, x, fprime = None)** implements both Newton's method (if the derivate of the function is given as **fprime**)  and the secant method for univariate rootfinding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.408293504279367"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def fun_d(x):\n",
    "    return np.cos(4 * (x - 0.25)) * 4 + 1 + 20 * x**19\n",
    "\n",
    "scipy.optimize.newton(fun,0.6, fun_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.40829350427936667"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scipy.optimize.newton(fun,0.6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We can use Jupyter's **%timeit** magic to compare the running time of each of the three methods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88.3 µs ± 12.9 µs per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit -n1 scipy.optimize.bisect(fun,0,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38.7 µs ± 14.9 µs per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit -n1 scipy.optimize.newton(fun,0.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "112 µs ± 65.5 µs per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit -n1 scipy.optimize.newton(fun,0.6, fun_d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "For comparison, our implementation of Newton's method above takes about the same running time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def my_newton(fun, fun_d, x, tol1 = 1e-8, tol2 = 1e-8):\n",
    "    \n",
    "    eps = 1\n",
    "    it = 0\n",
    "    maxit = 100\n",
    "    \n",
    "    while eps > tol1 and it < maxit:\n",
    "        it += 1\n",
    "        x_new = g_newton(fun, fun_d, x)\n",
    "        eps = abs(x - x_new)\n",
    "        x = x_new\n",
    "    \n",
    "    if abs(fun(x)) < tol2: \n",
    "        return x\n",
    "    else:\n",
    "        print(\"No solution found!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.40829350427936706"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_newton(fun, fun_d, 0.99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "123 µs ± 70.3 µs per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit -n1 my_newton(fun, fun_d, 0.6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Multiple Dimensions: NGM Revisited\n",
    "\n",
    "As an example for a multidimensional system of nonlinear equation, let's go back to our NGM model. Recall that a steady state is given by $(k_s, h_s)$ such that\n",
    "\n",
    "\\begin{equation}\n",
    "    \\left[\n",
    "    \\begin{array}{c}\n",
    "        S_1 \\\\\n",
    "        S_2\n",
    "    \\end{array}\n",
    "    \\right] =    \n",
    "    \\left[\n",
    "    \\begin{array}{c}\n",
    "        \\beta \\left[f_k(k_s, h_s) + 1 - \\delta \\right]  - 1 \\\\\n",
    "        \\left[ f(k_s, h_s) - \\delta k \\right]^{-\\nu} f_h(k_s, h_s) - B h_s^{\\eta}\n",
    "    \\end{array}\n",
    "    \\right] = \n",
    "    \\left[\n",
    "    \\begin{array}{c}\n",
    "        0 \\\\\n",
    "        0\n",
    "    \\end{array}\n",
    "    \\right]\n",
    "\\end{equation}\n",
    "\n",
    "To solve this system numerically, we first need to assign values to the model parameters. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "##### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "## utility\n",
    "beta = 0.8      # discount factor\n",
    "nu = 2       # risk-aversion coefficient for consumption\n",
    "eta = 1         # elasticity parameter for labor supply\n",
    "eps = 1e-6      # lower bound of consumption and labor supply\n",
    "## production\n",
    "alpha = 0.25\n",
    "delta = 0\n",
    "B = 0.8\n",
    "A = 1.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "##### Functions\n",
    "\n",
    "Next, it will be useful to define some auxiliary functions that implement the Cobb-Douglas production function, as well as its first and second derivatives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def cobb_douglas(x, alpha, A):\n",
    "    \"\"\"\n",
    "    Evaluates the Cobb-Douglas function with coefficient alpha and shift parameter A, for two inputs (x)\n",
    "    \"\"\"\n",
    "    return A * x[0]**alpha * x[1]**(1 - alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def cd_diff(x, alpha, A):\n",
    "    \"\"\"\n",
    "    Evaluates the first derivatives (returned as a tuple) of the Cobb-Douglas function with coefficient alpha and shift parameter A, for two inputs (x)\n",
    "    \"\"\"\n",
    "    return (A * alpha * cobb_douglas(x, alpha, A) / x[0], \n",
    "            A * (1 - alpha) * cobb_douglas(x, alpha, A) / x[1])\n",
    "\n",
    "def cd_diff2(x, alpha, A):\n",
    "    \"\"\"\n",
    "    Evaluates the second derivative (returned as a tuple, with the cross derivative as the last element) of the Cobb-Douglas function with coefficient alpha and shift parameter A, for two inputs (x)\n",
    "    \"\"\"\n",
    "    return (A * alpha * (alpha - 1) * cobb_douglas(x, alpha, A) / x[0]**2, \n",
    "            A * (1 - alpha) * (-alpha) * cobb_douglas(x, alpha, A) / x[1]**2,\n",
    "            A * alpha * (1 - alpha) * cobb_douglas(x, alpha, A) / (x[0] * x[1]) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Finally, we can code up the system of nonlinear equations $S$ as a Numpy array.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def steady(x):\n",
    "    \"\"\"\n",
    "    Returns the vector-valued function consisting of the steady-state conditions \n",
    "    \"\"\"\n",
    "    y = np.zeros(2)\n",
    "    mp = cd_diff(x, alpha, A)\n",
    "    \n",
    "    y[0] = beta * (mp[0] + 1 - delta) - 1\n",
    "    y[1] = (cobb_douglas(x, alpha, A) - delta * x[0])**(-nu) * mp[1] - B * x[1]**eta\n",
    "    \n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "For Newton's method, we also need to provide the Jacobian, i.e.\n",
    "\n",
    "\\begin{equation}\n",
    " J(k, h) = \\left[\n",
    "\\begin{matrix}\n",
    " \\partial S_1/ \\partial k &  \\partial S_1/ \\partial h \\\\\n",
    "  \\partial S_2/ \\partial k &  \\partial S_2 / \\partial h \n",
    "\\end{matrix}  \\right]\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def steady_jac(x):\n",
    "    \"\"\"\n",
    "    Returns the Jacobian of the vector-valued function consisting of the steady-state conditions \n",
    "    \"\"\"\n",
    "    J = np.zeros((2,2))\n",
    "    mp = cd_diff(x, alpha, A)\n",
    "    mp2 = cd_diff2(x, alpha, A)\n",
    "    \n",
    "    Q = cobb_douglas(x, alpha, A) - delta * x[0]\n",
    "    \n",
    "    J[0,0] = beta * mp2[0] \n",
    "    J[0,1] = beta * mp2[2]\n",
    "    J[1,1] = -nu * Q**(-nu-1) * mp[1]**2 + Q**(-nu) * mp2[1] - B * eta * x[1]**(eta - 1)\n",
    "    J[1,0] = -nu * Q**(-nu-1) * mp[1] * (mp[0] - delta) + Q**(-nu) * mp2[2] \n",
    "    \n",
    "    return J"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "##### Solve for the steady state\n",
    "\n",
    "Start by using our implementation of Newton's method written above. We also need to provide an initial guess $x_0$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of iterations = 10\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1.23548993, 0.95820563])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x0 = np.array([0.5, 0.5])\n",
    "my_newton_mult(steady, steady_jac, x0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Next, we use Scipy's **optimize.broyden1** function, an implementation of Broyden's method outlined above. As it is derivative-free, we do not have to provide the Jacobian: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.23548994 0.95820563]\n"
     ]
    }
   ],
   "source": [
    "res = scipy.optimize.broyden1(steady, x0, f_tol = 1e-8)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Note that alternatively, you can also call Scipy's **optimize.root** function, which is essentially a \"wrapper\" around different algorithms for solving nonlinear systems of equations, not only Broyden's method. \n",
    "\n",
    "I usually use the **root** function since it provides a more informative output, in particular on function values and number of iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     fun: array([ 7.35234096e-12, -1.00914832e-11])\n",
      " message: 'A solution was found at the specified tolerance.'\n",
      "     nit: 20\n",
      "  status: 1\n",
      " success: True\n",
      "       x: array([1.23548993, 0.95820563])\n"
     ]
    }
   ],
   "source": [
    "res = scipy.optimize.root(steady, x0,  tol = 1e-8, method = \"broyden1\")\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "As expected, Broyden's method takes more iterations to solve the system than Newton's method."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
